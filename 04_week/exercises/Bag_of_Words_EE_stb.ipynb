{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI3FUgI-QELJ"
      },
      "source": [
        "# 4.4. Bag-of-Words 🛍️\n"
      ],
      "id": "sI3FUgI-QELJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsfoZm-HQELL"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "SsfoZm-HQELL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFN6nQAHQELM"
      },
      "source": [
        "## Warmup"
      ],
      "id": "SFN6nQAHQELM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZBWmcOwQELM"
      },
      "source": [
        "#### In groups, research to find three different products/use cases of text classification that you find useful in the real world"
      ],
      "id": "eZBWmcOwQELM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   To summarize texts or scientific papers\n",
        "2.   sentiments analysis of for instance tweets\n",
        "3.sentiment analysis\n",
        "Summarisation  of texts https://www.paper-digest.com\n",
        "\n",
        "deepl translation \n",
        "\n",
        "Classification of any text (e.g. scientific articles)\n",
        "\n",
        "Chatbots\n",
        "\n",
        "Hiring and Recruitment --> CV screening\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0KaHZ85_Qh_4"
      },
      "id": "0KaHZ85_Qh_4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYBNqQ44QELN"
      },
      "source": [
        "## How do we process natural language in Machine Learning?"
      ],
      "id": "xYBNqQ44QELN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y40GV5MQELO"
      },
      "source": [
        "* The project this week is to classify text. How is this data different from other classification tasks you have seen so far?\n"
      ],
      "id": "9y40GV5MQELO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwIwv8WmQELO"
      },
      "source": [
        "* What pre-processing may we need to do to text to make it be somehow understandable to an ML algorithm?\n"
      ],
      "id": "jwIwv8WmQELO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a53JTvK9QELP"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "a53JTvK9QELP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs4MX8SgQELP"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "qs4MX8SgQELP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp2biaveQELQ"
      },
      "source": [
        "### Text Preprocessing"
      ],
      "id": "mp2biaveQELQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQfuY4nyQELR"
      },
      "source": [
        "In order to get better results and use fewer resources, some steps that we typically take in NLP text preprocessing include:\n",
        "\n",
        "- **Tokenisation:** each *document* (= string) in the *corpus* (=list of strings) is split into meaningful substrings called *tokens*. \n",
        "    - These are use-case and language-dependent, so eg. in English a token could be any substring surrrounded by whitespace, but say, in German it may be more useful to split up composite nouns further *(\"Rechtsschutzversicherungsgesellschaften\")*. \n",
        "    - To preserve some meaningful word order/context, it is possible to split the text into bi-grams, n-grams\n",
        "    - \"the cat eats the mouse\" becomes:\n",
        "        - [\"the\", \"cat\", \"eats\", \"the\", \"mouse\"] with word-based tokens\n",
        "        - [\"the cat\", \"cat eats\", \"eats the\", \"the mouse\"] with bi-grams\n",
        "        - three words for tri-grams etc.\n",
        "       \n",
        "- Cleaning the text:\n",
        "    - **Punctuation** should be removed.\n",
        "    - **Capitalisation** is dealt with\n",
        "    - Normalisation: **Stemming/Lemmatisation**:\n",
        "        - **Stemming** is the reduction of the word to its (pseudo)stem by removing suffixes via some heuristic rules. Does not always result in a real word at the end\n",
        "            - chang**es**, chang**ing**, chang**ed**, chang**ingly** (definitely a real word!) --> **chang**\n",
        "        - **Lemmatisation** is the conversion of a word to its dictionary form (\"lemma\"):\n",
        "            - went, going, goes, gone, go --> go\n",
        "        - Stemming is faster and less computationally expensive. Lemmatisation is more advanced and can preserve more meaning. Results from both have overlaps.\n",
        "        - A word can have multiple lemmas depending on the part of speech. Useful to combine lemmatisation with **Part-Of-Speech (POS) tagging**. Many packages have this included. \n",
        "    - Removing **stopwords**: some words are common and add almost no meaning, they are removed to save time and memory (eg. \"the\" can be removed from all documents basically without losing information. Can use preexisting stopwords included in python packages and/or add your own\n",
        "        - Can also remove **common words** that appear in >X% of your documents (eg. if your documents are all about water, it may be worth removing \"water\" from your corpus as it also doesn't add information in this case \n",
        "- Vectorisation:\n",
        "    - In order to have our data in a format that can be used in Machine Learning, it must be somehow numeric. There are several ways of vectorising our text, two of which we will look at today:\n",
        "        - **Bag of Words**\n",
        "        - **TF-IDF**          \n"
      ],
      "id": "AQfuY4nyQELR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKuteuaPQELR"
      },
      "source": [
        "**Once we have our text data in its preprocessed state, what could we use as features to train our model?**\n",
        "\n"
      ],
      "id": "SKuteuaPQELR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXmQDKhxQELS"
      },
      "source": [
        "## What is Bag of Words (BoW)?"
      ],
      "id": "oXmQDKhxQELS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekdMLlBfQELS"
      },
      "source": [
        "* Text vectorization is the process of converting each piece of text in the corpus into a numerical vector\n",
        "* There are various ways to do this: Bag of Words, TF-IDF, Neural Networks, ...etc.\n",
        "\n",
        "\n",
        "### Bag of Words:\n",
        "\n",
        "The first attempt at creating word vectors.\n",
        "The common approach for word vectorization until 2013 (when recurrent NN were proposed by Mikolov et al)."
      ],
      "id": "ekdMLlBfQELS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4AB9jwcQELS"
      },
      "source": [
        "![1*hLvya7MXjsSc3NS2SoLMEg.png](attachment:1*hLvya7MXjsSc3NS2SoLMEg.png)"
      ],
      "id": "r4AB9jwcQELS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzAolMiqQELT"
      },
      "source": [
        "> We take all the words in our vocabulary and assign them a column in a matrix. We count the number of occurences of that word in each document (=row of the matrix)."
      ],
      "id": "uzAolMiqQELT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hr51-9yQELT"
      },
      "source": [
        "## Bag of Words with song lyrics"
      ],
      "id": "4Hr51-9yQELT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5mHynMZQELU"
      },
      "source": [
        "### Let's collect a very small corpus of song lyrics!\n",
        "\n",
        "* Artist 1: Bob Marley\n",
        "\n",
        "\n",
        "* Artist 2: Eminem\n",
        "\n",
        "\n"
      ],
      "id": "I5mHynMZQELU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkYPmmTnQELU"
      },
      "outputs": [],
      "source": [
        "# X: before preprocessing, feature engineering and vectorisation\n",
        "corpus= [\"excuse me while I light my spliff which I love\",             # Bob Marley\n",
        "          \"no woman no cry I love spliffs\",        # Bob Marley\n",
        "          \"I'm slim shady and I sometimes cry cry cry in love\",      # Eminem\n",
        "          \"Palms are sweaty, love vomit on sweater, woman's spaghetti\"]   # Eminem\n",
        "# y:\n",
        "labels = [\"Bob Marley\"] * 2 + [\"Eminem\"] * 2"
      ],
      "id": "CkYPmmTnQELU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pgo7CKHQELW"
      },
      "outputs": [],
      "source": [
        "labels"
      ],
      "id": "3Pgo7CKHQELW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvzdw8YYQELW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "id": "xvzdw8YYQELW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHxUTYXUQELW"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
      ],
      "id": "JHxUTYXUQELW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQI7aQ29QELX"
      },
      "outputs": [],
      "source": [
        "X_df"
      ],
      "id": "VQI7aQ29QELX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6i7_VUvQELX"
      },
      "source": [
        "`CountVectoriser` automatically transforms the strings to lowercase and strips accents. It has an in-built token regex pattern that it will use as the default unless another is specified."
      ],
      "id": "m6i7_VUvQELX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyEVgRUzQELX"
      },
      "outputs": [],
      "source": [
        "X_df.shape"
      ],
      "id": "gyEVgRUzQELX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ymCfCHQELY"
      },
      "source": [
        "This is why it is called a bag of words: we have lost all word order and semantic relationships. The representation of \"the cat ate the mouse\" and \"the mouse ate the cat\" are exactly the same."
      ],
      "id": "0_ymCfCHQELY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxmO68gzQELY"
      },
      "source": [
        "####  What is our X?"
      ],
      "id": "VxmO68gzQELY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69WeyjweQELY"
      },
      "outputs": [],
      "source": [
        "X"
      ],
      "id": "69WeyjweQELY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gutStHXqQELY"
      },
      "outputs": [],
      "source": [
        "type(X)"
      ],
      "id": "gutStHXqQELY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx1ofK13QELY"
      },
      "source": [
        "A **sparse matrix** is one where most of the entries are zero. As this can get very large very fast, SciPy stores only the locations of the nonzero elements plus their values and none of the zeros "
      ],
      "id": "nx1ofK13QELY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c37w5xa_QELZ"
      },
      "outputs": [],
      "source": [
        "#print(X)"
      ],
      "id": "c37w5xa_QELZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4NIbFKnQELZ"
      },
      "source": [
        "To return the **dense** (= not sparse) version of the matrix, we use the `.todense()` method:"
      ],
      "id": "Q4NIbFKnQELZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So_le4P9QELZ"
      },
      "outputs": [],
      "source": [
        "X.todense()"
      ],
      "id": "So_le4P9QELZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnh556qQELZ"
      },
      "source": [
        "From just four very short snippets of text with repeating words we already have a dataframe with 24 columns. As our corpus grows, so will the size of our vocabulary. This becomes an issue with both storage and computational cost. This is why we need to make our vocabulary as small as possible while preserving as much useful information as we can."
      ],
      "id": "rHnh556qQELZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnzUM80bQELZ"
      },
      "source": [
        "How can we remove the most common words?\n",
        "\n",
        "1. Using a list of stop words\n",
        "2. Removing the words that appear in more than X% of documents\n",
        "\n",
        "\n",
        "- See `CountVectorizer` documentation for which parameters to use: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "- After running `CountVectorizer` use `.vocabulary_` and `.stop_words_` attributes on the `vectorizer` variable to see which words have remained and which are removed (latter attribute only works in the case of the second method). Alternatively, you can also do this by inspecting `X_df`."
      ],
      "id": "FnzUM80bQELZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-Dtz76MQELa"
      },
      "outputs": [],
      "source": [
        "#Using stopwords\n",
        "#english is built-in, can substitute for any list of stopwords in any language\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
      ],
      "id": "3-Dtz76MQELa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZzJKQFWQELa"
      },
      "outputs": [],
      "source": [
        "X_df"
      ],
      "id": "OZzJKQFWQELa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4ap1_BFQELa"
      },
      "outputs": [],
      "source": [
        "X_df.shape"
      ],
      "id": "X4ap1_BFQELa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nlqNkp6QELa"
      },
      "outputs": [],
      "source": [
        "# Remove words that are in 80% of documents\n",
        "vectorizer = CountVectorizer(max_df=0.8)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), index=labels)"
      ],
      "id": "_nlqNkp6QELa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPew9-T7QELb"
      },
      "outputs": [],
      "source": [
        "X_df"
      ],
      "id": "LPew9-T7QELb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1kUalJKQELb"
      },
      "outputs": [],
      "source": [
        "vectorizer.stop_words_ \n",
        "\n",
        "# Use-case specific stop-words that are detected"
      ],
      "id": "D1kUalJKQELb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzSP12zWQELb"
      },
      "source": [
        "## Normalisation using TF-IDF"
      ],
      "id": "NzSP12zWQELb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEp7mAlZQELb"
      },
      "source": [
        "* This normalizes the word counts in our BOW and aims to address the popularity/frequency of words in the whole corpus (not just inside a single document).\n"
      ],
      "id": "aEp7mAlZQELb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0mxi8QVQELb"
      },
      "source": [
        "**TF-IDF**\n",
        "* TF - Term Frequency (% count of a word  𝑤  in doc  𝑑 )\n",
        "* IDF - Inverse Document Frequency\n",
        "\n",
        "$$TF\\cdot IDF = TF(w,d) \\cdot IDF(w)$$\n",
        "\n",
        "$$IDF(w) = log(\\dfrac{1+  \\text{n.documents}}{1 + \\text{n.documents containing word w}})+1$$\n",
        "\n"
      ],
      "id": "d0mxi8QVQELb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHFWhaqUQELb"
      },
      "source": [
        "![1*qQgnyPLDIkUmeZKN2_ZWbQ.png](attachment:1*qQgnyPLDIkUmeZKN2_ZWbQ.png)"
      ],
      "id": "YHFWhaqUQELb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxi5zAESQELb"
      },
      "source": [
        "**The steps for calculating TFIDF are:**\n",
        "\n",
        "For each vector:\n",
        "1. Calculate the term frequency for each term in the vector\n",
        "2. Calculate the inverse doc frequency for each term in the vector\n",
        "3. Multiply the two for each term in the vector\n",
        "4. Then normalise each vector by the Euclidean norm `(numpy.linalg.norm)`\n",
        "\n",
        "$$norm = \\dfrac{v}{||v||^2}$$"
      ],
      "id": "Bxi5zAESQELb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUH1sjkIQELc"
      },
      "source": [
        "We can do this in `sklearn` and add it to our pipeline:"
      ],
      "id": "dUH1sjkIQELc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGrbAFfgQELc"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tf = TfidfTransformer() \n",
        "#Use the CountVectorised data\n",
        "X_norm = tf.fit_transform(X)"
      ],
      "id": "cGrbAFfgQELc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeuFbNicQELc"
      },
      "outputs": [],
      "source": [
        "X_norm"
      ],
      "id": "DeuFbNicQELc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfFk0UMkQELc"
      },
      "outputs": [],
      "source": [
        "X_norm_df=pd.DataFrame(X_norm.todense(), columns=vectorizer.get_feature_names(), index=labels)\n",
        "X_norm_df"
      ],
      "id": "DfFk0UMkQELc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vb0u3Q0QELd"
      },
      "source": [
        "### How do we put it all together?"
      ],
      "id": "0vb0u3Q0QELd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "digsUW1WQELd"
      },
      "source": [
        "**Option 1:** go through all the steps outlined above: preprocess-tokenise-lemmatise-vectorise-tfidf"
      ],
      "id": "digsUW1WQELd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j1c2WeBQELd"
      },
      "source": [
        "**Option 2:** use sklearn's `TfVectorizer` class to combine CountVectorizer and TfIdfTransformer in one "
      ],
      "id": "3j1c2WeBQELd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-hp5cUiQELd"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "id": "m-hp5cUiQELd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKvATmu_QELe"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "id": "CKvATmu_QELe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0XrxGt1QELe"
      },
      "outputs": [],
      "source": [
        "X_vec = vectorizer.fit_transform(corpus)\n",
        "X_vec_df = pd.DataFrame(X_vec.todense(), columns=vectorizer.get_feature_names(), index=labels)"
      ],
      "id": "V0XrxGt1QELe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vVQYKjVQELe"
      },
      "outputs": [],
      "source": [
        "X_vec_df"
      ],
      "id": "5vVQYKjVQELe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-6ku6H3QELe"
      },
      "source": [
        "What happens if our test set has unseen words in it? As long as we `.fit_transform()` our `TfidfVectorizer`on the training data only and `.transform()` our test data, any words not in the original training vocabulary will be ignored.\n"
      ],
      "id": "0-6ku6H3QELe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol6GQQk6QELe"
      },
      "outputs": [],
      "source": [
        "test_data=['hello I love my dog but not when it vomits']\n",
        "test_vec=vectorizer.transform(test_data)"
      ],
      "id": "Ol6GQQk6QELe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HAFx_XuQELf"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(test_vec.todense(), columns=vectorizer.get_feature_names())"
      ],
      "id": "4HAFx_XuQELf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "slOjckvkQELf"
      },
      "source": [
        "## Next steps:\n"
      ],
      "id": "slOjckvkQELf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "mQQea7X5QELf"
      },
      "source": [
        "- Pre-Process and vectorise your lyrics corpus\n",
        "- Can you draw some interesting insights from your data?\n",
        "- Can you run a classification algorithm (or two) on your data to predict the artist? How would you make sure to preprocess the test data in exactly the same way? How does it perform on unseen lyrics?\n",
        "- Bonus if you're done: see if you can create a wordcloud ([Guide here](https://spiced.space/euclidean-eukalyptus/ds-course/chapters/project_lyrics/README.html)) to visualise each artist's lyrics\n",
        "- Another Bonus: try the LDA topic modelling exercise in the [course notes](https://spiced.space/naive-zatar/ds-course/chapters/project_lyrics/bag_of_words/README.html)\n"
      ],
      "id": "mQQea7X5QELf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "u-7oYZqBQELf"
      },
      "source": [
        "## References"
      ],
      "id": "u-7oYZqBQELf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8vsw2P2RQELf"
      },
      "source": [
        "- [An explanation of Stemming/Lemmatisation from Stanford with several stemming examples](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
        "- [Nice article on Lemmatisation](https://devopedia.org/lemmatization)\n",
        "- [Comparison of several commonly used tokenisers/lemmatisers](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
        "- [What is WordNet](https://wordnet.princeton.edu/)\n",
        "- [Lemmatisation function](https://gist.github.com/MaxHalford/68b584e9154098151e6d9b5aa7464948) adapted in notes\n",
        "- [Top 5 Word Tokenizers That Every NLP Data Scientist Should Know](https://towardsdatascience.com/top-5-word-tokenizers-that-every-nlp-data-scientist-should-know-45cc31f8e8b9)\n",
        "-[Great explainer of TF-IDF](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)\n",
        "- [Another nice article on calculating TF-IDF](https://towardsdatascience.com/a-gentle-introduction-to-calculating-the-tf-idf-values-9e391f8a13e5)\n",
        "\n",
        "**German Language Resources**\n",
        "- [German Language NLP with NLTK examples](https://data-dive.com/german-nlp-binary-text-classification-of-reviews-part1)\n",
        "- [German models with SpaCy](https://spacy.io/models/de) - see the [SpaCy lesson in the course materials](https://spiced.space/naive-zatar/ds-course/chapters/project_lyrics/spacy/README.html) too\n",
        "- [Common pitfalls with the preprocessing of German text for NLP](https://medium.com/idealo-tech-blog/common-pitfalls-with-the-preprocessing-of-german-text-for-nlp-3cfb8dc19ebe)\n"
      ],
      "id": "8vsw2P2RQELf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "GqknaX2BQELf"
      },
      "source": [
        "### Bonus content! "
      ],
      "id": "GqknaX2BQELf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "mlAES_D_QELg"
      },
      "outputs": [],
      "source": [
        "# If you are interested in what sklearn's english stopwords are:\n",
        "from sklearn.feature_extraction import stop_words\n",
        "print(stop_words.ENGLISH_STOP_WORDS)"
      ],
      "id": "mlAES_D_QELg"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Bag_of_Words_EE_stb.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}