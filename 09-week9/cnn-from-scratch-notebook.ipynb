{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nnfs","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:23:31.464898Z","iopub.execute_input":"2022-02-26T16:23:31.466309Z","iopub.status.idle":"2022-02-26T16:23:43.040789Z","shell.execute_reply.started":"2022-02-26T16:23:31.466150Z","shell.execute_reply":"2022-02-26T16:23:43.039898Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nnfs","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:47:07.833209Z","iopub.execute_input":"2022-02-26T16:47:07.833593Z","iopub.status.idle":"2022-02-26T16:47:07.839812Z","shell.execute_reply.started":"2022-02-26T16:47:07.833558Z","shell.execute_reply":"2022-02-26T16:47:07.838325Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# neuronA\n# basic concept of an input/output neurons connection\n\ninputs = [1, 2, 3] #unique input from three neurons from prev. layer\nweights = [0.2, 0.8,-0.5] #weights per node\nbias = 2\n\n# first step to add up all inputs x weights + bias\n\noutput = inputs[0]*weights[0] + inputs[1]*weights[1] +inputs[2]*weights[2] + bias\noutput","metadata":{"execution":{"iopub.status.busy":"2022-02-25T09:20:15.006879Z","iopub.execute_input":"2022-02-25T09:20:15.007879Z","iopub.status.idle":"2022-02-25T09:20:15.016648Z","shell.execute_reply.started":"2022-02-25T09:20:15.007817Z","shell.execute_reply":"2022-02-25T09:20:15.015869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# neuronB\n# basic concept of an input/output neurons connection\n\ninputs = [1, 2, 3, 2.5] #unique input from three neurons (e.g. from prev. layer)\n\nweights1 = [0.2, 0.8,-0.5, 1.0]\nweights2 = [0.5, -0.91, 0.26, -0.5]\nweights3 = [-0.26, -0.27, 0.17, 0.87]#weights per node\n\nbias1 = 2\nbias2 = 3\nbias3 = 0.5\n\n# as before add up all inputs x weights + bias. \n# However this time, each node will have its own unique weights and bias\n# inputs remain the same as nodes do not change\n\noutput = [inputs[0]*weights1[0] + inputs[1]*weights1[1] +inputs[2]*weights1[2] +inputs[3]*weights1[3]+ bias1,\n         inputs[0]*weights2[0] + inputs[1]*weights2[1] +inputs[2]*weights2[2] +inputs[3]*weights2[3]+ bias2,\n         inputs[0]*weights3[0] + inputs[1]*weights3[1] +inputs[2]*weights3[2] +inputs[3]*weights3[3]+ bias3]\noutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# neuronB simplified via lists & loops\n\ninputs = [1, 2, 3, 2.5] #unique input from three neurons (e.g. from prev. layer)\n\nweights = [[0.2, 0.8,-0.5, 1.0],\n           [0.5, -0.91, 0.26, -0.5],\n           [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n\nlayer_outputs = [] # output of current layer\n\nfor neuron_weights, neuron_bias in zip(weights, biases):\n    \n    neuron_output = 0 # output of given neuron\n    \n    for n_input, weight in zip(inputs, neuron_weights):\n        neuron_output += n_input*weight\n    \n    neuron_output += neuron_bias\n    layer_outputs.append(neuron_output)\n    \nlayer_outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-25T10:29:51.956444Z","iopub.execute_input":"2022-02-25T10:29:51.957198Z","iopub.status.idle":"2022-02-25T10:29:51.974001Z","shell.execute_reply.started":"2022-02-25T10:29:51.957156Z","shell.execute_reply":"2022-02-25T10:29:51.97281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# neuronB simplified via numpy's dot product \n\ninputs = [1, 2, 3, 2.5] #unique input from three neurons (e.g. from prev. layer)\n\nweights = [[0.2, 0.8,-0.5, 1.0],\n           [0.5, -0.91, 0.26, -0.5],\n           [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2, 3, 0.5]\n\n\noutput = np.dot(weights, inputs) + biases\noutput","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:28:33.848335Z","iopub.execute_input":"2022-02-25T12:28:33.848906Z","iopub.status.idle":"2022-02-25T12:28:33.860498Z","shell.execute_reply.started":"2022-02-25T12:28:33.848856Z","shell.execute_reply":"2022-02-25T12:28:33.8595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding an additional layer (of weights and biases) to neuronB \n\ninputs = [[1, 2, 3, 2.5],\n          [2.0, 5.0, -1.0, 2.0],\n          [-1.5, 2.7, 3.3, -0.8]]\n\n#first layer \nweights1 = [[0.2, 0.8,-0.5, 1.0],\n           [0.5, -0.91, 0.26, -0.5],\n           [-0.26, -0.27, 0.17, 0.87]]\n\nbiases1 = [2, 3, 0.5]\n\n#second layer\nweights2 = [[0.1, -0.14,0.5],\n           [-0.5, 0.12, -0.33],\n           [-0.44, 0.73, -0.13]]\n\nbiases2 = [-1, 2, -0.5]\n\n\nlayer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n\n#input of layer2 taken from output of layer1\nlayer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n\nlayer2_outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:15:29.842199Z","iopub.execute_input":"2022-02-25T13:15:29.842489Z","iopub.status.idle":"2022-02-25T13:15:29.854367Z","shell.execute_reply.started":"2022-02-25T13:15:29.84246Z","shell.execute_reply":"2022-02-25T13:15:29.853468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multilayer neural network now converted to objects to enable simple build-up\n\n#input becomes X dataset (3 samples), will not change\nX = [[1, 2, 3, 2.5],\n     [2.0, 5.0, -1.0, 2.0],\n     [-1.5, 2.7, 3.3, -0.8]]\n\nnp.random.seed(0)\n\n#when loading a saved model, essentially just loading pre-defined weights and biases\n#here we will initialise the weights and biases directly\nclass Layer_Dense:\n    def __init__(self,n_inputs, n_neurons):\n        self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n\nlayer1 = Layer_Dense(4,5)\nlayer2 = Layer_Dense(5,2)\n\nlayer1.forward(X)\nprint(layer1.output)\nlayer2.forward(layer1.output)\nprint(layer2.output)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T13:46:21.324955Z","iopub.execute_input":"2022-02-25T13:46:21.325748Z","iopub.status.idle":"2022-02-25T13:46:21.336098Z","shell.execute_reply.started":"2022-02-25T13:46:21.325712Z","shell.execute_reply":"2022-02-25T13:46:21.33519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activation Functions in long code\n\ninputs = [0, 2, -1, 3.3,-2.7, 1.2, 2.2, -100]\noutput = [] \n\n# for i in inputs:\n#     if i > 0:\n#         output.append(i)\n#     elif i <= 0:\n#         output.append(0)\n        \n# or ...\n\nfor i in inputs:\n    output.append(max(0,i))\n               \noutput","metadata":{"execution":{"iopub.status.busy":"2022-02-25T18:37:05.696029Z","iopub.execute_input":"2022-02-25T18:37:05.69653Z","iopub.status.idle":"2022-02-25T18:37:05.703847Z","shell.execute_reply.started":"2022-02-25T18:37:05.696478Z","shell.execute_reply":"2022-02-25T18:37:05.70305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding ReLU to my neural network framework\nfrom nnfs.datasets import spiral_data\n\n# replaces random seed and sets default datatype for numpy\nnnfs.init()\n\n#generates 100 feature sets of three classes\nX, y = spiral_data(100,3)\n\nclass Layer_Dense:\n    ''' defining the weights/biases and running multilayered CNN'''\n    def __init__(self,n_inputs, n_neurons):\n        self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n\nclass Activation_ReLU:\n    ''' defining activation function to run on each node. All negatives become zero'''\n    def forward(self, inputs):\n        self.output = np.maximum(0, inputs)\n\n#spiral dataset just x,y coords so inputs \"2\"        \nlayer1 = Layer_Dense(2,5)\nactivation1 = Activation_ReLU()\n\nlayer1.forward(X)\nactivation1.forward(layer1.output)\nactivation1.output","metadata":{"execution":{"iopub.status.busy":"2022-02-25T18:57:56.413051Z","iopub.execute_input":"2022-02-25T18:57:56.414114Z","iopub.status.idle":"2022-02-25T18:57:56.434426Z","shell.execute_reply.started":"2022-02-25T18:57:56.414054Z","shell.execute_reply":"2022-02-25T18:57:56.433438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basics to Softmax Function of output layer in long code\nimport math\n\nlayer_outputs = [4.8, 1.21, 2.385]\n\nE = math.e\n\nexp_values = []\n\nfor output in layer_outputs:\n    exp_values.append(E**output)\n    \n#now we normalize to get probability as value\n\nnorm_base = sum(exp_values)\nnorm_values = []\n\nfor value in exp_values:\n    norm_values.append( value / norm_base )\n    \nnorm_values, sum(norm_values)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:31:45.216918Z","iopub.execute_input":"2022-02-25T19:31:45.217513Z","iopub.status.idle":"2022-02-25T19:31:45.226518Z","shell.execute_reply.started":"2022-02-25T19:31:45.217455Z","shell.execute_reply":"2022-02-25T19:31:45.225691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting above softmax function into numpy functions\n\nlayer_outputs = [4.8, 1.21, 2.385]\nE = math.e\n\nexp_values = np.exp(layer_outputs)\nnorm_values = exp_values / np.sum(exp_values)\n    \nnorm_values, sum(norm_values)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:34:32.178533Z","iopub.execute_input":"2022-02-25T19:34:32.178847Z","iopub.status.idle":"2022-02-25T19:34:32.188448Z","shell.execute_reply.started":"2022-02-25T19:34:32.178817Z","shell.execute_reply":"2022-02-25T19:34:32.187557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating softmax function to run on batches of data (not just single output vector)\n\nlayer_outputs = [[4.8, 1.21, 2.385],\n                 [8.9, -1.81, 0.2],\n                 [1.41, 1.051, 0.026]]\n\n#numpy automatically runs iteratively on all elements\nexp_values = np.exp(layer_outputs)\n\n#axis adds info on what layer to act on, and not just sum all as single value\n#keepdims allows matrix to keep same dimension when summing\nnp.sum(layer_outputs, axis=1, keepdims=True)\n\nnorm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T19:45:57.017868Z","iopub.execute_input":"2022-02-25T19:45:57.018238Z","iopub.status.idle":"2022-02-25T19:45:57.027938Z","shell.execute_reply.started":"2022-02-25T19:45:57.018184Z","shell.execute_reply":"2022-02-25T19:45:57.026942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding softmax activation output layer to our CNN\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n\nclass Layer_Dense:\n    ''' defining the weights/biases and running multilayered CNN'''\n    def __init__(self,n_inputs, n_neurons):\n        self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n\nclass Activation_ReLU:\n    ''' defining activation function to run on each node. All negatives become zero'''\n    def forward(self, inputs):\n        self.output = np.maximum(0, inputs)\n        \nclass Activation_Softmax:\n    ''' defining softmax function'''\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n\n#setting the data scene\nX, y = spiral_data(samples=100, classes=3)\n\ndense1 = Layer_Dense(2,3)\nactivation1 = Activation_ReLU()\n#input should match output of dense1, which is 3\ndense2 = Layer_Dense(3,3)\nactivation2 = Activation_Softmax()\n\n#running our network on X\ndense1.forward(X)\nactivation1.forward(dense1.output)\n\ndense2.forward(activation1.output)\nactivation2.forward(dense2.output)\n\nactivation2.output[:5]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T22:15:41.064264Z","iopub.execute_input":"2022-02-25T22:15:41.064622Z","iopub.status.idle":"2022-02-25T22:15:41.099024Z","shell.execute_reply.started":"2022-02-25T22:15:41.064583Z","shell.execute_reply":"2022-02-25T22:15:41.098248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coding the categorical cross-entropy loss function without numpy\n\nimport math\n\nsoftmax_output = [0.7, 0.1, 0.2]\ntarget_output = [1,0,0]\n\n# CCE just the sum of each -log of the softmax output of the target class\nloss = -(math.log(softmax_output[0])* target_output[0] +\n         math.log(softmax_output[1])* target_output[1] +\n         math.log(softmax_output[2])* target_output[2]\n        )\n\n# or...since the calculated output of incorrect classes is zero\nloss = -math.log(softmax_output[0])\nloss1 = -math.log(0.7)\n\nloss, loss1","metadata":{"execution":{"iopub.status.busy":"2022-02-25T22:50:31.229153Z","iopub.execute_input":"2022-02-25T22:50:31.229539Z","iopub.status.idle":"2022-02-25T22:50:31.239415Z","shell.execute_reply.started":"2022-02-25T22:50:31.229487Z","shell.execute_reply":"2022-02-25T22:50:31.238788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coding the CCE as confidence of each prediction from a batch (without numpy)\n\n'''\nClasses: 0-->Dog, 1-->Cat, 2-->Human\nclass_targets in words [dog, cat, cat]\n'''\n#typically a batch of outputs as we run a batch of inputs\nsoftmax_outputs = [[0.7, 0.1, 0.2], #<--dog, 0.7\n                  [0.1, 0.5, 0.4],  #<--cat, 0.5\n                  [0.02, 0.9, 0.08]]#<--cat, 0.9\n\nclass_targets = [0,1,1]\n\nfor target_index, distribution in zip(class_targets, softmax_outputs):\n    print(distribution[target_index])\n    \n    \n'''\nthe above loops through the lists as\n(0,[0.7,0.1,0.2])\n(1,[0.1,0.5,0.4])\n(1,[0.02,0.9,0.08])\n\nand takes the first element to use as the index for the resp. list\n''' ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T23:15:15.310027Z","iopub.execute_input":"2022-02-25T23:15:15.310912Z","iopub.status.idle":"2022-02-25T23:15:15.318364Z","shell.execute_reply.started":"2022-02-25T23:15:15.310861Z","shell.execute_reply":"2022-02-25T23:15:15.317521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coding the CCE as confidence of each prediction WITH numpy\n\n#converting softmax_outputs to numpy array enables us to perfrom same task in single line of code and call upon elements more easily\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],\n                            [0.1, 0.5, 0.4],\n                            [0.02, 0.9, 0.08]])\n\nclass_targets = [0,1,1]\n\n#returns an array of each of the losses from batch\nneg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n\n'''\nthe above code returns an array with each element calculated as below\n-np.log(softmax_outputs[0,0])\n-np.log(softmax_outputs[1,1])\n-np.log(softmax_outputs[2,1])\n'''\n\n#average loss values gives an idea of how CNN performs as a whole. Can also sum losses too\naverage_loss = np.mean(neg_log)\naverage_loss","metadata":{"execution":{"iopub.status.busy":"2022-02-25T23:24:04.970612Z","iopub.execute_input":"2022-02-25T23:24:04.97101Z","iopub.status.idle":"2022-02-25T23:24:04.981794Z","shell.execute_reply.started":"2022-02-25T23:24:04.970933Z","shell.execute_reply":"2022-02-25T23:24:04.98115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coding the CCE as confidence of each prediction WITH numpy\n\n#the above code works but we will run into trouble when calc. log loss of zero (infinite)\n#this will occur when the confidence of the correct class is zerp\n# a single zero is technically correct (it is infinitely wrong!) but it will result in all losses/probs being infinite throughout the CNN after averaging and backprop\n\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],\n                            [0.1, 0.5, 0.4],\n                            [0.02, 0.9, 0.08]])\n\nclass_targets = [0,1,1]\n\nneg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n\n#we can handle zero/infinity problems by clipping all predicted values by an insignificant value that is close to zero but not quite zero\n#this handles problem of inifinite values without introducing a bias\n\ny_pred_clip = np.clip(y_pred, 1e-7, 1 - 1e-7)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T12:22:35.692938Z","iopub.execute_input":"2022-02-26T12:22:35.693241Z","iopub.status.idle":"2022-02-26T12:22:35.702756Z","shell.execute_reply.started":"2022-02-26T12:22:35.693203Z","shell.execute_reply":"2022-02-26T12:22:35.70193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Introducing the categorical class entropy 'loss function' to our CNN\n\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n\nclass Layer_Dense:\n    ''' defining the weights/biases and running multilayered CNN'''\n    def __init__(self,n_inputs, n_neurons):\n        self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n        self.biases = np.zeros((1, n_neurons))\n    def forward(self, inputs):\n        self.output = np.dot(inputs, self.weights) + self.biases\n\nclass Activation_ReLU:\n    ''' defining activation function to run on each node. All negatives become zero'''\n    def forward(self, inputs):\n        self.output = np.maximum(0, inputs)\n        \nclass Activation_Softmax:\n    ''' defining softmax function'''\n    def forward(self, inputs):\n        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n        self.output = probabilities\n        \nclass Loss:\n    ''' defining CCE loss function calculations'''\n    def calculate(self, output, y):\n        '''output will be output from model, y the intended target values'''\n        sample_losses = self.forward(output,y)\n        data_loss = np.mean(sample_losses)\n        return data_loss\n\nclass Loss_CategoricalCrossentropy(Loss):\n    '''\n    CCE loss function inherited from base Loss class\n    also handles both scaler, 1d array of [0,1], and OHC, 2d array of [[1,0],[0,1]], target classes as input\n    see youtube sentdex 8/9 11:06 -  13:48 for further explanation\n    '''\n    def forward(self,y_pred, y_true): \n        '''y_pred model output, y_true target training class values'''\n        samples = len(y_pred)\n        y_pred_clip = np.clip(y_pred, 1e-7, 1-1e-7)\n        \n        if len(y_true.shape) ==1:\n            ''' then scaler values placed as class'''\n            correct_confidences = y_pred_clip[range(samples), y_true]\n        \n        elif len(y_true.shape) == 2:\n            correct_confidences = np.sum(y_pred_clip * y_true, axis=1)\n        \n        neg_log_likelihoods = -np.log(correct_confidences)\n        return neg_log_likelihoods\n      \n#setting the data scene\nX, y = spiral_data(samples=100, classes=3)        \n        \ndense1 = Layer_Dense(2,3)\nactivation1 = Activation_ReLU()\n#input should match output of dense1, which is 3\ndense2 = Layer_Dense(3,3)\nactivation2 = Activation_Softmax()\n\n#running our network on X\ndense1.forward(X)\nactivation1.forward(dense1.output)\n\ndense2.forward(activation1.output)\nactivation2.forward(dense2.output)\n\n#print(activation2.output[:5])\n\nloss_function = Loss_CategoricalCrossentropy()\nloss = loss_function.calculate(activation2.output,y)\n\nprint(f\"Loss: {loss}\")        ","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:48:39.720691Z","iopub.execute_input":"2022-02-26T16:48:39.721620Z","iopub.status.idle":"2022-02-26T16:48:39.753221Z","shell.execute_reply.started":"2022-02-26T16:48:39.721566Z","shell.execute_reply":"2022-02-26T16:48:39.751878Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-26T16:48:51.044326Z","iopub.execute_input":"2022-02-26T16:48:51.044988Z","iopub.status.idle":"2022-02-26T16:48:51.052211Z","shell.execute_reply.started":"2022-02-26T16:48:51.044940Z","shell.execute_reply":"2022-02-26T16:48:51.051400Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}